{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GooglePalmEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_files(data_folder):\n",
    "    files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.pdf')]\n",
    "    docs = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(file)  # Use PyPDFLoader for PDF files\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file}: {e}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=load_files(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2=chunk_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\Rag-app\\ragenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\yores\\AppData\\Local\\Temp\\ipykernel_15176\\2741019295.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='models\\embedding-model')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='models\\embedding-model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_qa_mini = HuggingFaceEmbeddings(model_name=\"models\\embedding-model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model=pipeline(\"question-answering\", model=\"models/roberta-qa\", tokenizer=\"models/roberta-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def store_in_vector_store(embeddings, docs):\n",
    "    index_path = \"faiss_index\"\n",
    "\n",
    "    # Check if the FAISS index exists\n",
    "    if os.path.exists(index_path):\n",
    "        # Load the FAISS vector store from the disk\n",
    "        vector_store = FAISS.load_local(index_path, embeddings,allow_dangerous_deserialization=True)\n",
    "        print(\"FAISS index loaded from disk.\")\n",
    "    else:\n",
    "        # If it doesn't exist, create a new vector store from documents\n",
    "        vector_store = FAISS.from_documents(docs, embeddings)\n",
    "        # Save it for future use\n",
    "        vector_store.save_local(index_path)\n",
    "        print(\"FAISS index created and saved to disk.\")\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"faiss_index\"):\n",
    "        # Load the FAISS vector store from the disk\n",
    "        vector_store = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization=True)\n",
    "        print(\"FAISS index loaded from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "step3=store_in_vector_store(embeddings,step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import (\n",
    "    ContextualCompressionRetriever,\n",
    "   \n",
    "    MergerRetriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(vector_store,embeddings):\n",
    "    # Define Individual Retrievers\n",
    "    retriever_tns = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5, \"include_metadata\": False}\n",
    "    )\n",
    "\n",
    "    retriever_ad = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5, \"include_metadata\": False}\n",
    "    )\n",
    "\n",
    "    # Merge Retrievers\n",
    "    lotr = MergerRetriever(retrievers=[retriever_tns, retriever_ad])\n",
    "\n",
    "    # Add Embedding-Based Filtering\n",
    "    filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "\n",
    "    # Create a Document Compression Pipeline\n",
    "    pipeline = DocumentCompressorPipeline(transformers=[filter])\n",
    "\n",
    "    # Contextual Compression Retriever\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline, base_retriever=lotr\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "# Function to Use the Compression Retriever\n",
    "def retrieve_documents(query,compression_retriever):\n",
    "    return compression_retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4=retrieval(vector_store,multi_qa_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Tell me about the economic consequences of COVID-19?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yores\\AppData\\Local\\Temp\\ipykernel_6884\\2227228818.py:35: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return compression_retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "step5=retrieve_documents(query,step4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "# os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_key:\n",
    "    raise ValueError(\"MISTRAL_API_KEY is not set in the .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import BaseLanguageModel\n",
    "\n",
    "class WrappedMistral(BaseLanguageModel):\n",
    "    def __init__(self, mistral_instance):\n",
    "        self.mistral_instance = mistral_instance\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        return self.mistral_instance.generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    api_key=api_key\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "def generation(compression_retriever,chatllm):\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                    return_messages=True)\n",
    "\n",
    "    qa_withmemory = ConversationalRetrievalChain.from_llm(chatllm,\n",
    "                                            compression_retriever,\n",
    "                                            memory=memory)\n",
    "    return qa_withmemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I love programming. = J'aime la programmation.\\n\\nHere's a breakdown:\\n- I love = J'aime\\n- programming = la programmation\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 27, 'total_tokens': 66, 'completion_tokens': 39}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-2f56749d-d850-4d34-8d26-467c897c1cf8-0', usage_metadata={'input_tokens': 27, 'output_tokens': 39, 'total_tokens': 66})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yores\\AppData\\Local\\Temp\\ipykernel_15176\\4113485653.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\",\n"
     ]
    }
   ],
   "source": [
    "step6=generation(step4,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Tell me about the economic consequences of COVID-19?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yores\\AppData\\Local\\Temp\\ipykernel_15176\\4163398543.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  x=step6(query)\n"
     ]
    }
   ],
   "source": [
    "x=step6(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Tell me about the economic consequences of COVID-19?',\n",
       " 'chat_history': [HumanMessage(content='Tell me about the economic consequences of COVID-19?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The global economy experienced a significant downturn during the COVID-19 pandemic. According to the International Monetary Fund (IMF), the world economy contracted by 3.5% in 2020. Small and medium-sized enterprises (SMEs), which form the backbone of many economies, were disproportionately affected by lockdowns and restrictions. In the United States alone, an estimated 200,000 businesses permanently closed during the first year of the pandemic.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'The global economy experienced a significant downturn during the COVID-19 pandemic. According to the International Monetary Fund (IMF), the world economy contracted by 3.5% in 2020. Small and medium-sized enterprises (SMEs), which form the backbone of many economies, were disproportionately affected by lockdowns and restrictions. In the United States alone, an estimated 200,000 businesses permanently closed during the first year of the pandemic.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The global economy experienced a significant downturn during the COVID-19 pandemic. According to the International Monetary Fund (IMF), the world economy contracted by 3.5% in 2020. Small and medium-sized enterprises (SMEs), which form the backbone of many economies, were disproportionately affected by lockdowns and restrictions. In the United States alone, an estimated 200,000 businesses permanently closed during the first year of the pandemic.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
